# ShadowMesh v11 UDP Frame Handler Performance Investigation
## BMAD Team Deep Dive - Critical Performance Bottleneck

**Date**: 2025-11-05
**Version**: v11-chr001 (TUN + UDP)
**Status**: ðŸ”´ CRITICAL - 95% packet loss, 3000ms latency
**Investigation Focus**: `cmd/lightnode-l3-v11/main.go:290-327` (forwardTUNPacketsUDP function)

---

## Executive Summary

v11 TUN-based Layer 3 implementation exhibits severe performance degradation:
- **ICMP Packet Loss**: 95% (1 of 20 packets received)
- **Latency**: 3009ms average (vs expected <10ms)
- **UDP Transport**: âœ… Confirmed functional (tcpdump shows bidirectional traffic)
- **Root Cause**: Performance bottleneck in TUNâ†’UDP forwarding loop

**Architecture is sound. Performance tuning required.**

---

## Current Performance Metrics

### ICMP Test Results (v11 TUN devices)

```bash
# Test 1: Initial (3 packets)
$ ping -c 3 10.10.10.3
3 packets transmitted, 1 received, 66.67% packet loss
rtt min/avg/max/mdev = 1748.898/1748.898/1748.898/0.000 ms

# Test 2: Extended (20 packets)
$ ping -c 20 10.10.10.3
20 packets transmitted, 1 received, 95% packet loss
rtt min/avg/max/mdev = 3008.898/3008.898/3008.898/0.000 ms, pipe 3
```

### UDP Transport Layer (CONFIRMED WORKING)

```bash
# tcpdump on both TUN devices
UK TUN0: ICMP echo request and reply both captured
Belgium TUN0: Multiple requests and 1 reply captured
UDP Port 9443: Bidirectional traffic confirmed with ephemeral source ports
```

**Conclusion**: UDP forwarding mechanism works correctly. Problem is in application-level frame processing.

---

## Performance Bottleneck Analysis

### Target Function: `forwardTUNPacketsUDP` (main.go:290-327)

```go
func forwardTUNPacketsUDP(tun *layer3.TUNInterface, udpConn *p2p.UDPConnection) {
    var packetCount uint64
    for {
        if !tun.IsActive() || !udpConn.IsActive() {
            break
        }

        startTotal := time.Now()

        // TUN read (IP packets at Layer 3)
        startTUNRead := time.Now()
        packet, err := tun.ReadPacket()  // âš ï¸ BLOCKING I/O
        if err != nil {
            if tun.IsActive() {
                log.Printf("TUN read error: %v", err)
            }
            continue
        }
        tunReadDuration := time.Since(startTUNRead)

        // UDP send (includes frame header + UDP write)
        startUDPSend := time.Now()
        err = udpConn.SendFrame(packet)
        if err != nil {
            log.Printf("Failed to send packet via UDP: %v", err)
            continue
        }
        udpSendDuration := time.Since(startUDPSend)
        totalDuration := time.Since(startTotal)

        // Log timing every 100th packet
        packetCount++
        if packetCount%100 == 0 {
            log.Printf("[PROFILE-TUN-UDP-%s] Total=%v TUNRead=%v UDPSend=%v PacketSize=%d",
                udpConn.GetPeerID(), totalDuration, tunReadDuration, udpSendDuration, len(packet))
        }
    }
}
```

### Critical Performance Issues Identified

#### 1. **Blocking Synchronous I/O (PRIMARY ROOT CAUSE)**

**Location**: Line 301 `packet, err := tun.ReadPacket()`

**Problem**:
- Tight loop with blocking read from TUN device
- No buffering mechanism between TUN and UDP
- Each packet processed sequentially (no concurrency)
- ICMP reply packets may be delayed waiting for TUN read to complete

**Evidence**:
```
File: pkg/layer3/tun.go:148-164
func (t *TUNInterface) ReadPacket() ([]byte, error) {
    packet := make([]byte, 1500)  // âš ï¸ Memory allocation on every read
    n, err := t.iface.Read(packet)  // âš ï¸ Blocking system call
    if err != nil {
        return nil, fmt.Errorf("failed to read packet: %w", err)
    }
    return packet[:n], nil  // âš ï¸ Allocation + copy
}
```

**Impact**:
- If TUN device blocks waiting for packets, UDP receive path also blocked
- Bidirectional communication suffers (ping request â†’ reply cycle broken)
- Latency accumulates with each blocking read

#### 2. **Memory Allocation Overhead**

**Location**: `tun.go:155` `packet := make([]byte, 1500)`

**Problem**:
- New 1500-byte buffer allocated for EVERY packet
- No buffer pool or reuse mechanism
- Go GC pressure increases with packet rate
- Memory allocation itself adds microseconds per packet

**Comparison to v10 TAP implementation** (`cmd/lightnode-l2-v10/main.go:290-327`):
```go
// v10 uses identical structure - same performance issues expected
frame, err := tap.ReadFrame()  // Same blocking pattern
```

**NOTE**: v10 TAP also had ICMP issues (100% loss documented in V10_ARCHITECTURE_EVALUATION.md), suggesting this is a known limitation of the synchronous I/O approach.

#### 3. **No Concurrent Processing**

**Problem**:
- Single goroutine handles both TUNâ†’UDP and UDPâ†’TUN directions
- Actually TWO goroutines (one per direction), but no buffering between them
- If one direction blocks, the other continues, but bidirectional flows suffer

**Current Structure**:
```go
// Goroutine 1: TUN â†’ UDP (forwardTUNPacketsUDP)
go forwardTUNPacketsUDP(tunInterface, udpConn)

// Goroutine 2: UDP â†’ TUN (setupUDPReceiver's frame handler)
udpConn.SetFrameHandler(func(packet []byte) {
    err := tunInterface.WritePacket(packet)  // âš ï¸ Also blocking
    // ...
})
udpConn.StartReceiving()
```

**Issue**: Both directions use blocking I/O without buffering. ICMP echo reply may be written to TUN while TUN reader is blocked, causing delivery delays.

#### 4. **UDP Frame Overhead**

**Location**: `pkg/p2p/udp_connection.go:73-119`

**Structure**: [8 bytes seq][2 bytes size][N bytes packet]

**Overhead per packet**:
- 10 bytes header (acceptable)
- Sequence number atomic increment (negligible)
- Frame building: `make([]byte, 10+len(frame))` + `copy()` - **ALLOCATION**

**Evidence of allocation overhead**:
```go
packet := make([]byte, 10+len(frame))  // Line 95
binary.BigEndian.PutUint64(packet[0:8], seq)
binary.BigEndian.PutUint16(packet[8:10], frameSize)
copy(packet[10:], frame)  // Line 99
```

Every UDP send allocates a new buffer for framing. At high packet rates (ICMP replies), this accumulates.

#### 5. **No Backpressure or Flow Control**

**Problem**:
- If UDP socket buffer fills, `WriteToUDP` blocks
- No mechanism to signal TUN reader to slow down
- Packets may be dropped silently by kernel if application can't keep up

**Linux UDP Socket Buffers**:
```go
// From udp_connection.go:46-48
conn.SetReadBuffer(8 * 1024 * 1024)   // 8MB
conn.SetWriteBuffer(8 * 1024 * 1024)  // 8MB
```

Buffers are large (8MB), but with 1500-byte packets:
- 8MB / 1500 bytes â‰ˆ 5,461 packets buffered
- At 3000ms latency, that's ~1.8 packets/sec throughput
- **This matches observed ICMP behavior** (1 of 20 packets = 5% success rate)

---

## Performance Comparison: v11 TUN vs v10 TAP

### v10 TAP (Layer 2) Performance

**From V10_ARCHITECTURE_EVALUATION.md**:
- âœ… UDP forwarding at Layer 2: Microsecond-level latency
- âŒ ICMP routing: 100% packet loss (kernel Layer 3 limitation, NOT UDP issue)
- âœ… Profile logs: 3-150Âµs per frame (confirmed bidirectional flow)

**Verdict**: v10 UDP mechanism was FAST. Layer 3 routing was the problem.

### v11 TUN (Layer 3) Performance

- âœ… ICMP routing: Fixed rp_filter, packets DO reach TUN devices
- âŒ ICMP throughput: 95% packet loss (architectural bottleneck)
- âŒ Latency: 3000ms (1000x worse than v10's microseconds)

**Conclusion**: Switching to TUN fixed the routing issue but introduced a severe performance regression. The synchronous I/O pattern that worked acceptably at Layer 2 (TAP) does not scale to Layer 3 (TUN) ICMP traffic.

---

## Root Cause: Synchronous I/O Without Buffering

### Why This Pattern Fails for ICMP

**ICMP Echo Flow**:
1. Ping client sends ICMP echo request â†’ TUN device
2. `forwardTUNPacketsUDP` reads request from TUN (blocking)
3. Request forwarded via UDP to remote peer
4. Remote peer processes request, generates ICMP echo reply
5. Reply arrives via UDP, **frame handler writes to TUN**
6. But `forwardTUNPacketsUDP` is still blocked waiting for NEXT TUN packet!
7. Reply sits in kernel TUN buffer until next read cycle
8. By the time reply is processed, ping client has timed out

**Timing Analysis**:
```
T+0ms:    Ping sends request
T+0ms:    forwardTUNPacketsUDP reads request (blocks on TUN)
T+1ms:    Request sent via UDP
T+2ms:    Remote peer receives request
T+3ms:    Remote peer sends reply via UDP
T+4ms:    Reply arrives, frame handler writes to TUN
T+????:   forwardTUNPacketsUDP STILL BLOCKED waiting for next TUN packet
T+3000ms: Ping timeout, reports packet loss
```

**This explains the 3000ms latency**: The reply IS delivered (tcpdump confirms), but the TUN reader is blocked, so the reply sits in kernel buffer until a timeout/retry triggers another read cycle.

### Why v10 TAP Had Similar Issues

v10 TAP architecture (documented in `V10_ARCHITECTURE_EVALUATION.md:156-186`) had:
- âœ… UDP forwarding: Microsecond-level latency (Layer 2 frames)
- âŒ ICMP routing: 100% packet loss (kernel couldn't route Layer 3 through TAP + userspace UDP tunnel)

**Key difference**: v10's problem was kernel routing at Layer 3. v11's problem is application-level performance at Layer 3.

---

## Proposed Solutions (Priority Order)

### Solution 1: Buffered Channels for Asynchronous I/O (HIGHEST PRIORITY)

**Goal**: Decouple TUN read/write from UDP send/receive using Go channels

**Implementation**:
```go
func forwardTUNPacketsUDP(tun *layer3.TUNInterface, udpConn *p2p.UDPConnection) {
    // Buffered channel for packets (TUN â†’ UDP direction)
    tunToUDP := make(chan []byte, 256)  // Buffer 256 packets

    // Goroutine 1: TUN reader (non-blocking produces to channel)
    go func() {
        for {
            if !tun.IsActive() {
                close(tunToUDP)
                return
            }
            packet, err := tun.ReadPacket()
            if err != nil {
                if tun.IsActive() {
                    log.Printf("TUN read error: %v", err)
                }
                continue
            }
            select {
            case tunToUDP <- packet:
                // Packet buffered successfully
            default:
                // Channel full, drop packet (backpressure)
                log.Printf("TUNâ†’UDP channel full, dropping packet")
            }
        }
    }()

    // Goroutine 2: UDP sender (consumes from channel)
    for packet := range tunToUDP {
        if !udpConn.IsActive() {
            break
        }
        err := udpConn.SendFrame(packet)
        if err != nil {
            log.Printf("Failed to send packet via UDP: %v", err)
        }
    }
}
```

**Benefits**:
- TUN reader never blocks on UDP send
- UDPâ†’TUN frame handler can write immediately without waiting for TUN read
- Buffering absorbs burst traffic (ICMP echo reply can be delivered while waiting for next request)
- Backpressure mechanism (channel full = drop packet, signal congestion)

**Estimated Impact**: Should reduce latency from 3000ms to <50ms, increase throughput from 5% to >80%

### Solution 2: Buffer Pool for Memory Allocation Reduction (HIGH PRIORITY)

**Goal**: Eliminate per-packet memory allocations

**Implementation**:
```go
// Global buffer pool
var packetBufferPool = sync.Pool{
    New: func() interface{} {
        b := make([]byte, 1500)
        return &b
    },
}

func (t *TUNInterface) ReadPacket() ([]byte, error) {
    bufPtr := packetBufferPool.Get().(*[]byte)
    buf := *bufPtr

    n, err := t.iface.Read(buf)
    if err != nil {
        packetBufferPool.Put(bufPtr)  // Return buffer on error
        return nil, fmt.Errorf("failed to read packet: %w", err)
    }

    // Allocate result buffer (caller owns)
    result := make([]byte, n)
    copy(result, buf[:n])

    packetBufferPool.Put(bufPtr)  // Return buffer to pool
    return result, nil
}
```

**Benefits**:
- Reduces GC pressure (fewer allocations)
- Faster packet processing (no malloc overhead)
- More predictable latency (less GC pauses)

**Estimated Impact**: 10-20% latency reduction, smoother throughput

### Solution 3: Pre-allocated Frame Buffers for UDP Send (MEDIUM PRIORITY)

**Goal**: Eliminate UDP frame allocation per packet

**Implementation**:
```go
func (u *UDPConnection) SendFrame(frame []byte) error {
    seq := atomic.AddUint64(&u.sequenceNum, 1)
    frameSize := uint16(len(frame))

    // Use stack-allocated array for small frames
    var stackBuf [1510]byte  // 10 bytes header + 1500 bytes MTU
    packet := stackBuf[:10+len(frame)]

    binary.BigEndian.PutUint64(packet[0:8], seq)
    binary.BigEndian.PutUint16(packet[8:10], frameSize)
    copy(packet[10:], frame)

    _, err := u.conn.WriteToUDP(packet, u.remoteAddr)
    return err
}
```

**Benefits**:
- No heap allocation for UDP frame (stack only)
- Faster than sync.Pool for small, fixed-size buffers
- Zero GC impact

**Estimated Impact**: 5-10% latency reduction

### Solution 4: Parallel Processing with Worker Pools (LOW PRIORITY - OVERKILL?)

**Goal**: Process multiple packets concurrently

**Implementation**: Questionable benefit given UDP ordering requirements and overhead of goroutine spawning. **NOT RECOMMENDED** unless Solutions 1-3 are insufficient.

---

## Comparison to Industry Standards

### WireGuard Approach (for reference)

WireGuard uses:
- **Kernel-space implementation** (no userspace I/O at all)
- **Lock-free queues** for packet buffering
- **NAPI polling** for batch packet processing
- **Result**: Near-line-rate performance (40+ Gbps)

ShadowMesh is userspace, so we can't match WireGuard's performance, but we CAN achieve acceptable performance (<10ms latency, <1% packet loss) with proper buffering.

### Tailscale Approach (userspace like ShadowMesh)

Tailscale (built on WireGuard):
- Uses **kernel WireGuard** for data path
- Userspace only for control plane (NAT traversal, etc.)
- **Result**: Same performance as WireGuard

ShadowMesh uses userspace for data path (TUN + UDP), so we need explicit buffering to compensate.

---

## Testing Plan

### Phase 1: Implement Buffered Channels (Solution 1)

1. Modify `forwardTUNPacketsUDP` to use buffered channels
2. Rebuild v11-chr001 binaries
3. Deploy to both servers
4. Disable rp_filter on chr001 interface (same as tun0)
5. Run ICMP test: `ping -c 100 10.10.10.3`
6. **Success Criteria**: <10% packet loss, <100ms average latency

### Phase 2: Implement Buffer Pool (Solution 2)

1. Add `sync.Pool` to `pkg/layer3/tun.go`
2. Rebuild and redeploy
3. Run extended ICMP test: `ping -c 1000 10.10.10.3`
4. **Success Criteria**: <5% packet loss, <50ms average latency

### Phase 3: UDP Frame Optimization (Solution 3)

1. Use stack-allocated buffers in `SendFrame`
2. Rebuild and redeploy
3. Run iperf3 tests: TCP and UDP throughput
4. **Success Criteria**: >100 Mbps TCP throughput, <1% UDP packet loss

### Phase 4: Baseline Comparison (v11 vs v6)

1. Deploy v6 (TCP-only implementation) for comparison
2. Run same tests on v6 (reported 5.2s latency, 0% loss in docs)
3. Compare results
4. **Success Criteria**: v11 performance matches or exceeds v6

---

## Recommended Action Plan

**Immediate Priority** (next 2-4 hours):

1. âœ… Deploy chr001-branded binaries (v11 with branding fix)
2. ðŸ”´ **CRITICAL**: Implement Solution 1 (buffered channels) - highest ROI
3. ðŸŸ¡ Test ICMP with Solution 1 applied
4. ðŸŸ¢ If successful, proceed to Solution 2 (buffer pool)
5. ðŸŸ¢ Run full performance test suite (ICMP, TCP, UDP)
6. ðŸŸ¢ Compare against v6 baseline

**Timeline**:
- Solution 1 implementation: 30-45 minutes
- Build + deploy: 10 minutes
- Testing: 30 minutes
- Solution 2 implementation: 20-30 minutes
- Final testing: 1 hour
- **Total**: 2.5-3 hours to production-ready v11

---

## Risk Assessment

### Risks of Implementing Solutions

**Solution 1 (Buffered Channels)**:
- Risk: Channel buffer size tuning (too small = drops, too large = memory)
- Mitigation: Start with 256 packets (384KB), monitor drop rate, adjust
- Risk: Packet reordering if multiple goroutines send
- Mitigation: Single sender goroutine per direction (current design)

**Solution 2 (Buffer Pool)**:
- Risk: Buffer leak if not properly returned to pool
- Mitigation: Defer pattern + careful error handling
- Risk: Race conditions if buffer reused before copy complete
- Mitigation: Always copy packet data before returning buffer

**Solution 3 (Stack Buffers)**:
- Risk: Stack overflow if packet larger than MTU
- Mitigation: Bounds checking, fallback to heap for oversized packets
- Risk: Incompatibility with certain network drivers
- Mitigation: Test on both ARM64 and AMD64 architectures

### Risks of NOT Implementing Solutions

- v11 remains unusable (95% packet loss)
- Project deadline missed
- Wasted effort on v11 refactoring (TUN migration)
- Must revert to v10 or v6 (losing Layer 3 benefits)

**Verdict**: Solutions 1-2 are LOW RISK, HIGH REWARD. Must implement.

---

## Conclusion

v11 architecture is fundamentally sound:
- âœ… TUN device creation works (chr001 branding applied)
- âœ… UDP transport layer functional (tcpdump proves bidirectional traffic)
- âœ… TCP control plane operational
- âœ… ICMP routing fixed (rp_filter disabled)

The performance bottleneck is **not architectural**, it's **implementational**:
- Synchronous blocking I/O without buffering
- Per-packet memory allocations
- No backpressure mechanism

**These are all solvable with standard Go patterns (buffered channels, sync.Pool, stack allocation).**

**Recommendation**: Proceed with Solution 1 (buffered channels) immediately. This will likely resolve 80-90% of the performance issue. Solutions 2-3 are optimizations for the remaining 10-20%.

**Estimated time to production-ready v11**: 2.5-3 hours

---

## QA Sign-Off

**BMAD QA Evaluation**: âš ï¸ **URGENT ACTION REQUIRED**

**Blockers**: Performance bottleneck in forwardTUNPacketsUDP (synchronous I/O without buffering)

**Next Steps**:
1. Implement buffered channels (Solution 1) - **HIGHEST PRIORITY**
2. Test ICMP performance (target: <10% loss, <100ms latency)
3. Implement buffer pool (Solution 2) if needed
4. Run full performance test suite
5. Compare against v6 baseline

**Quality Gate Status**: ðŸŸ¡ YELLOW (architecture sound, implementation needs tuning)

---

## Appendix: Detailed Code Analysis

### A1. TUN Read Path Analysis

**File**: `pkg/layer3/tun.go:148-164`

```go
func (t *TUNInterface) ReadPacket() ([]byte, error) {
    if !t.isActive {
        return nil, fmt.Errorf("TUN device is closed")
    }

    // âš ï¸ ISSUE 1: Allocation on every read
    packet := make([]byte, 1500)  // 1500 bytes allocated

    // âš ï¸ ISSUE 2: Blocking system call
    n, err := t.iface.Read(packet)
    if err != nil {
        return nil, fmt.Errorf("failed to read packet: %w", err)
    }

    // âš ï¸ ISSUE 3: Another allocation + copy for slice
    return packet[:n], nil  // Returns slice pointing to allocated buffer
}
```

**Performance Impact**:
- malloc() call: ~50-100ns (varies with GC state)
- System call (read): ~500-1000ns (context switch overhead)
- Total per packet: ~1Âµs minimum

At 1000 packets/sec (moderate load), this is acceptable. At 10,000+ packets/sec (high load), this becomes a bottleneck.

**ICMP Issue**: Ping at 1 packet/sec should be fine, BUT the blocking read means bidirectional flows are serialized, causing replies to wait in kernel buffer.

### A2. UDP Send Path Analysis

**File**: `pkg/p2p/udp_connection.go:73-119`

```go
func (u *UDPConnection) SendFrame(frame []byte) error {
    // ... mutex and validation ...

    seq := atomic.AddUint64(&u.sequenceNum, 1)  // âœ… Fast (no lock)

    // âš ï¸ ISSUE 1: Allocation on every send
    frameSize := uint16(len(frame))
    packet := make([]byte, 10+len(frame))  // 10 + 1500 = 1510 bytes

    // âœ… Fast (direct memory write)
    binary.BigEndian.PutUint64(packet[0:8], seq)
    binary.BigEndian.PutUint16(packet[8:10], frameSize)

    // âš ï¸ ISSUE 2: Memory copy
    copy(packet[10:], frame)  // 1500 bytes copied

    // âš ï¸ ISSUE 3: System call (may block if socket buffer full)
    _, err := u.conn.WriteToUDP(packet, remoteAddr)
    return err
}
```

**Performance Impact**:
- malloc(): ~50-100ns
- copy(): ~500ns for 1500 bytes (depends on CPU cache)
- WriteToUDP: ~500-1000ns (if buffer available)
- Total: ~1.5-2Âµs per packet

Comparable to TUN read, but cumulative across both directions.

### A3. UDP Receive Path Analysis

**File**: `pkg/p2p/udp_connection.go:133-213`

```go
func (u *UDPConnection) receiveLoop() {
    // âœ… Single allocation for receive buffer
    buffer := make([]byte, 65535)  // Max UDP packet size

    for {
        // ... active check ...

        // âš ï¸ Blocking read (waits for UDP packet)
        n, remoteAddr, err := u.conn.ReadFromUDP(buffer)
        if err != nil {
            continue
        }

        // ... remote address learning ...

        // âœ… Parse header (no allocation)
        seq := binary.BigEndian.Uint64(buffer[0:8])
        frameSize := binary.BigEndian.Uint16(buffer[8:10])

        // âš ï¸ ISSUE: Allocation for extracted frame
        frame := make([]byte, frameSize)
        copy(frame, buffer[10:10+frameSize])

        // âš ï¸ CRITICAL: Frame handler writes to TUN (MAY BLOCK)
        u.frameHandler(frame)  // This calls tunInterface.WritePacket()
    }
}
```

**Key Issue**: Frame handler calls `tunInterface.WritePacket()`, which is a blocking write to TUN device. If kernel TUN buffer is full, this blocks, preventing more UDP packets from being received.

**This is the BIDIRECTIONAL DEADLOCK**:
1. TUN reader blocked waiting for outgoing packets
2. UDP receiver blocked waiting to write incoming packets to TUN
3. Incoming ICMP reply can't be written to TUN
4. Deadlock until timeout/retry

### A4. Complete Call Stack Analysis

**Outbound Path** (ICMP echo request):
```
ping client â†’ TUN device (kernel) â†’ TUNInterface.ReadPacket() [BLOCKS]
â†’ forwardTUNPacketsUDP() â†’ udpConn.SendFrame() [may block on UDP socket]
â†’ Network â†’ Remote peer
```

**Inbound Path** (ICMP echo reply):
```
Remote peer â†’ Network â†’ UDP socket â†’ udpConn.receiveLoop()
â†’ frameHandler() â†’ TUNInterface.WritePacket() [BLOCKS waiting for TUN writer]
â†’ TUN device (kernel) â†’ ping client
```

**Deadlock Scenario**:
- forwardTUNPacketsUDP blocked on TUNInterface.ReadPacket() (waiting for outbound packet)
- frameHandler blocked on TUNInterface.WritePacket() (waiting for TUN to accept inbound packet)
- Kernel TUN device has packets in both directions but can't deliver because userspace is blocked
- After 3000ms timeout, kernel drops packets, forwardTUNPacketsUDP unblocks, cycle repeats

**This explains the 3000ms latency and 95% packet loss.**

---

## Appendix: References

- WireGuard whitepaper: https://www.wireguard.com/papers/wireguard.pdf
- Go sync.Pool documentation: https://pkg.go.dev/sync#Pool
- Linux TUN/TAP documentation: https://www.kernel.org/doc/Documentation/networking/tuntap.txt
- V10_ARCHITECTURE_EVALUATION.md (this repository)
- CLAUDE.md Project Guidelines (this repository)
